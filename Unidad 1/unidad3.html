<!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Redes neuronales y Deep Learning</title>
  <!-- MISMO CSS QUE LA PARTE 1 / ÁRBOLES -->
  <style>
    :root {
      --fg: #111827; /* slate-900 */
      --muted: #4b5563; /* gray-600 */
      --bg: #ffffff;
      --brand: #0ea5e9; /* sky-500 */
      --brand-ink: #075985; /* sky-800 */
      --card: #f8fafc; /* slate-50 */
      --code: #111827; /* slate-900 */
      --border: #e5e7eb; /* gray-200 */
      /* accent */
      --h1: #1d4ed8; /* blue-700 */
      --h2: #6d28d9; /* violet-700 */
      --h3: #0284c7; /* sky-600 */
      --section-underline: #dbeafe; /* blue-100 */
      --divider: #e5e7eb; /* gray-200 */
    }
    html, body { background: var(--bg); color: var(--fg); font-family: system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji"; }
    .page { max-width: 920px; margin: auto; padding: 2rem 1.25rem 6rem; line-height: 1.65; }
    header h1 { font-size: clamp(1.6rem, 2.5vw + 1rem, 2.4rem); margin: 0 0 .25rem; color: var(--h1); letter-spacing: -0.01em; }
    header p.lead { color: var(--muted); margin: 0 0 1.25rem; }
    .breadcrumbs { font-size: .9rem; color: var(--muted); margin-bottom: .5rem; }
    .breadcrumbs a { color: var(--muted); text-decoration: none; }
    nav.toc { background: var(--card); border: 1px solid var(--border); border-radius: .75rem; padding: 1rem; margin: 1.25rem 0 2rem; }
    nav.toc strong { display: block; margin-bottom: .5rem; }
    nav.toc a { display: block; color: var(--brand-ink); text-decoration: none; padding: .25rem 0; }
    h2 { margin-top: 2.25rem; font-size: 1.6rem; color: var(--h2); padding-bottom: .35rem; border-bottom: 1px solid var(--section-underline); }
    h3 { margin-top: 1.5rem; font-size: 1.25rem; color: var(--h3); }
    .page section + section { border-top: 1px solid var(--divider); margin-top: 2.25rem; padding-top: 2rem; }
    .note { background: #ecfeff; border-left: 4px solid var(--brand); padding: .75rem 1rem; border-radius: .5rem; }
    .card { background: var(--card); border: 1px solid var(--border); border-radius: .75rem; padding: 1rem; }
    code, pre { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; font-size: .95em; }
    pre { background: #0b1020; color: #e2e8f0; border-radius: .5rem; padding: 1rem; overflow: auto; }
    .pill { display: inline-block; border: 1px solid var(--border); border-radius: 999px; padding: .1rem .6rem; font-size: .875rem; color: var(--muted); background: #fff; }
    details { border: 1px solid var(--border); border-radius: .5rem; padding: .75rem 1rem; background: #fff; }
    details + details { margin-top: .75rem; }
    footer { margin-top: 3rem; font-size: .9rem; color: var(--muted); }
    .top { position: fixed; right: 1rem; bottom: 1rem; background: var(--brand); color: #fff; border-radius: 999px; padding: .6rem .8rem; text-decoration: none; box-shadow: 0 6px 18px rgba(2,132,199,.3); }
    figure.media { margin: 1rem 0 1.25rem; }
    figure.media img { max-width: 100%; height: auto; display: block; border-radius: .5rem; }
    figure.media figcaption { margin-top: .5rem; text-align: center; color: var(--muted); font-size: .95rem; }
  </style>
</head>
<body>
  <div class="page">
    <header id="top">
      <div class="breadcrumbs">Sistemas de Aprendizaje Automático → Tema 3</span></div>
      <h1>Redes neuronales y Deep Learning</h1>
    </header>

    <nav class="toc" aria-label="Tabla de contenidos">
      <strong>Índice de la página</strong>
      <a href="#intro-rn">1. Introducción</a>
      <a href="#neurona-artificial">2. Neurona artificial</a>
      <a href="#capas-arquitectura">3. Capas y arquitectura básica</a>
      <a href="#pixeles-imagenes">4. Ejemplo de píxeles e imágenes</a>
      <a href="#densas-vs-conv">5. Redes convolucionales</a>
      <a href="#perceptron-mlp">6. Perceptrón y redes MLP</a>
      <a href="#flujo-supervisado">7. Flujo básico de un modelo supervisado</a>
    </nav>

    <!-- 1 -->
    <section id="intro-rn">
      <h2>1. Introducción</h2>
      <p>
        Una <strong>red neuronal artificial</strong> es un modelo de aprendizaje automático inspirado, de forma muy simplificada,
        en cómo procesa la información el cerebro. En lugar de una neurona biológica, usamos muchas unidades matemáticas muy
        sencillas conectadas entre sí.
      </p>
      <p>
        El objetivo de una red neuronal es <strong>aprender una relación</strong> entre unos datos de entrada
        (imágenes, texto, números, sensores…) y una salida deseada (clase, valor numérico, probabilidad, etc.) a partir
        de ejemplos.
      </p>
      <ul>
        <li><strong>Clasificación:</strong> detectar tumores benignos/malignos, identificar spam, reconocer dígitos escritos a mano.</li>
        <li><strong>Regresión:</strong> estimar el precio de un piso, prever la demanda eléctrica, predecir el tiempo de entrega de un pedido.</li>
      </ul>
      <p>
        Ejemplos de uso de una red neuronal.
      </p>

      <figure class="media">
        <img src="../img/tema3-aplicaciones.JPG" alt="Ejemplos visuales de aplicaciones de redes neuronales" loading="lazy" decoding="async">
        <figcaption>Ejemplos de uso de redes neuronales.</figcaption>
      </figure>

      <div class="note">
        <strong>Idea clave:</strong> no programamos reglas a mano. Le damos muchos ejemplos etiquetados y la red
        <em>aprende sola</em> qué combinación de parámetros funciona mejor.
      </div>
    </section>

    <!-- 2 -->
    <section id="neurona-artificial">
      <h2>2. Neurona artificial</h2>
      <p>
        La unidad básica de una red es la <strong>neurona artificial</strong>. Podemos verla como una pequeña calculadora que:
      </p>
      <ol>
        <li>Recibe varios <strong>valores de entrada</strong> (características del problema).</li>
        <li>A cada entrada le da una <strong>importancia</strong> (un peso).</li>
        <li>Hace una combinación de todas ellas (una especie de media ponderada).</li>
        <li>Decide si se “activa” o no mediante una <strong>función de activación</strong> (por ejemplo, devuelve un número entre 0 y 1).</li>
      </ol>
      <p>
        Por ejemplo: si pensamos en un sistema que decide si conceder un préstamo, una neurona podría recibir como entradas:
        <em>ingresos mensuales</em>, <em>antigüedad en la empresa</em> y <em>nivel de deudas</em>. En función de estos valores
        y de sus pesos, la neurona produce un número que refleja lo “fiable” que le parece ese cliente.
      </p>

      <figure class="media">
        <img src="../img/neuronaArtificial.png" alt="Esquema de una neurona artificial con entradas, pesos y salida" loading="lazy" decoding="async">
        <figcaption>Espacio para un esquema simple de neurona artificial (entradas → combinación → activación → salida).</figcaption>
      </figure>

      <p class="note">
        En este tema nos centraremos en la <strong>intuición práctica</strong>: qué hace la neurona y para qué sirve.
      </p>
    </section>

    <!-- 3 -->
    <section id="capas-arquitectura">
      <h2>3. Capas y arquitectura básica</h2>
      <p>
        Una neurona aislada puede resolver problemas muy sencillos (separar en dos grupos “más grandes que” o “más pequeños
        que” algo). Para tareas reales necesitamos muchas neuronas trabajando juntas en <strong>capas</strong>.
      </p>
      <ul>
        <li>
          <strong>Capa de entrada:</strong> recibe directamente los datos (p. ej. cada píxel de una imagen, cada variable de un
          dataset tabular, etc.).
        </li>
        <li>
          <strong>Capas ocultas:</strong> combinan y transforman la información para detectar patrones intermedios.
        </li>
        <li>
          <strong>Capa de salida:</strong> genera la predicción final (clase, probabilidad, valor numérico…).
        </li>
      </ul>
      <p>
        En un problema de <strong>clasificación binaria</strong> (por ejemplo, “tumor benigno” vs “maligno”), la capa de salida
        suele tener una única neurona que genera un número entre 0 y 1 interpretado como probabilidad de la clase positiva.
      </p>

      <figure class="media">
        <img src="../img/redNeuronalSimple.png" alt="Arquitectura de red neuronal simple" loading="lazy" decoding="async">
        <figcaption>Arquitectura de red neuronal simple.</figcaption>
      </figure>

      <div class="note">
        <strong>Terminología:</strong> cuando hablamos de la <em>arquitectura</em> de una red nos referimos a cuántas
        capas tiene y cuántas neuronas hay en cada capa.
      </div>
    </section>
    <section id="Funciones de activación">
  <h3>3.1. Funciones de activación</h2>

  <p>
    Las <strong>funciones de activación</strong> son las encargadas de decidir cuánto se “activa”
    una neurona a partir de la suma de entradas que recibe. Sin ellas, una red neuronal sería
    simplemente una combinación lineal de datos (como una regresión lineal encadenada) y no
    podría aprender relaciones complejas.
  </p>

  <p>
    Dicho de forma sencilla: la función de activación transforma la salida de cada neurona
    para introducir <strong>no linealidad</strong>, limitar el rango de valores y ayudar a que la red
    pueda aprender patrones más complejos (curvas, fronteras de decisión complicadas, etc.).
  </p>

  <h4>Función sigmoide</h3>

  <p>
    La función <strong>sigmoide</strong> transforma cualquier valor de entrada en un número entre
    <strong>0 y 1</strong>. Su curva tiene forma de “S” suave: para valores muy negativos se acerca a 0,
    para valores muy positivos se acerca a 1, y en torno al 0 cambia más rápidamente.
  </p>

  <!-- Espacio para una imagen pequeña de la sigmoide -->
  <figure class="media">
        <img src="../img/sigmoide.png" alt="Función de activación: Sigmoide" loading="lazy" decoding="async">
        <figcaption>Función de activación: Sigmoide.</figcaption>
  </figure>

  <h5>¿Cuándo usar la sigmoide?</h4>
  <ul>
    <li>
      Es muy habitual en la <strong>capa de salida</strong> de modelos de
      <strong>clasificación binaria</strong> (dos clases: sí/no, 0/1…) porque su salida puede
      interpretarse como una <strong>probabilidad</strong>.
    </li>
    <li>
      También puede usarse en redes pequeñas donde el rango [0, 1] tenga sentido para la tarea.
    </li>
  </ul>

  <h5>Inconvenientes de la sigmoide</h4>
  <ul>
    <li>
      Para valores muy grandes o muy pequeños, la curva se “aplana” y el gradiente se vuelve
      casi cero. Esto provoca el problema de los
      <strong>gradientes que se desvanecen</strong> (vanishing gradient), lo que dificulta
      el entrenamiento en redes profundas.
    </li>
    <li>
      Sus salidas están entre 0 y 1, es decir, <strong>no está centrada en 0</strong>. Esto puede
      hacer que el entrenamiento sea algo más lento frente a otras funciones.
    </li>
  </ul>

  <h4>Función ReLU (Rectified Linear Unit)</h3>

  <p>
    La función <strong>ReLU</strong> es muy sencilla: si la entrada es negativa, devuelve 0; si es
    positiva, devuelve la propia entrada. Es decir, “corta” todos los valores negativos y deja
    pasar los positivos tal cual.
  </p>

  <!-- Espacio para una imagen pequeña de la ReLU -->
    <figure class="media">
        <img src="../img/ReLU.png" alt="Función de activación: ReLU" loading="lazy" decoding="async">
        <figcaption>Función de activación: Sigmoide.</figcaption>
  </figure>

  <h5>¿Cuándo usar ReLU?</h4>
  <ul>
    <li>
      Es la función de activación <strong>más utilizada en capas ocultas</strong> de redes profundas
      (redes densas y convolucionales).
    </li>
    <li>
      Es una buena opción por defecto cuando diseñamos una red neuronal moderna para visión,
      texto, etc.
    </li>
    <li>
      Al no “aplanarse” tanto como la sigmoide en valores positivos, suele entrenar
      <strong>más rápido</strong> y con menos problemas de gradientes que se desvanecen.
    </li>
  </ul>

  <h5>Inconvenientes de ReLU</h4>
  <ul>
    <li>
      Si una neurona recibe muchas veces valores negativos, su salida será siempre 0 y dejará
      de actualizarse. Es lo que se conoce como el problema de las
      <strong>“neuronas muertas”</strong> (dead ReLUs).
    </li>
    <li>
      Su salida no está acotada por arriba (puede crecer mucho), lo que a veces puede hacer
      que algunas neuronas tengan valores muy grandes y haya que vigilar la estabilidad
      numérica.
    </li>
  </ul>

  <p>
    En resumen: las funciones de activación son las que dan “vida” a la red. La sigmoide es
    útil cuando queremos salidas tipo probabilidad en [0, 1], y la ReLU se ha convertido en
    el estándar en capas ocultas por su sencillez y buen rendimiento en redes profundas.
  </p>

  <p class="note">
        Recurso interactivo:
        <a href="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.26847&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false" target="_blank" rel="noopener noreferrer">
          Red Neuronal Online
        </a>
      </p>

</section>

    <!-- 4 -->
    <section id="pixeles-imagenes">
      <h2>4. Ejemplo de píxeles e imágenes</h2>
      <p>
        Un ejemplo clásico para entender las redes neuronales es el reconocimiento de dígitos escritos a mano (0–9).
        Cada imagen puede verse como una <strong>matriz de píxeles</strong>, donde cada píxel indica si hay tinta (valor alto)
        o fondo (valor bajo).
      </p>
      <p>
        Si “aplanamos” esa matriz en un vector, cada componente del vector será una entrada de la red. La red debe aprender a
        detectar trazos, curvas y formas que diferencien, por ejemplo, un <strong>“3”</strong> de un <strong>“8”</strong>.
      </p>

      <figure class="media">
        <img src="../img/pixeles.JPG" alt="Dígito representado como matriz de píxeles" loading="lazy" decoding="async">
        <figcaption>Dígito representado como matriz de píxeles.</figcaption>
      </figure>

      <p>
        En modelos más avanzados (redes convolucionales) no solo miramos píxeles sueltos, sino pequeños bloques de la imagen
        que permiten detectar bordes, esquinas y patrones más complejos.
      </p>
    </section>

    <!-- 5 -->
    <section id="densas-vs-conv" class="card">
      <h2>5. Redes convolucionales (CNN)</h2>
      <p>
        Existen muchos tipos de redes neuronales. Las primeras clases nos centraremos en <strong>redes convolucionales</strong>.
      </p>
      <ul>
        <li>Están diseñadas para trabajar con <strong>datos en forma de matriz</strong> (imágenes, audio espectral, vídeo…).</li>
        <li>Aplican filtros pequeños que recorren la imagen y detectan patrones locales (bordes, texturas, formas).</li>
        <li>Permiten capturar estructura espacial y reducir mucho el número de parámetros.</li>
      </ul>

      <figure class="media">
        <img src="../img/conv.JPG" alt="Red Convolucional" loading="lazy" decoding="async">
        <figcaption>Red Neuronal Convolucional (CNN).</figcaption>
      </figure>

      <p class="note">
        Recurso interactivo:
        <a href="https://tensorspace.org/html/playground/index.html" target="_blank" rel="noopener noreferrer">
          Red convolucionales
        </a>
      </p>
    </section>

    <!-- 6 -->
    <section id="perceptron-mlp">
      <h2>6. Perceptrón y redes MLP</h2>
      <p>
        Para preparar la primera práctica en Python necesitamos entender dos conceptos: el <strong>perceptrón</strong> y
        las <strong>redes MLP</strong>.
      </p>

      <h3>6.1. Perceptrón</h3>
      <p>
        El perceptrón es una neurona artificial diseñada para separar datos en dos grupos:
        por ejemplo, decidir si un punto en 2D pertenece a la clase azul o a la clase naranja.
      </p>
      <ul>
        <li>Toma varias entradas (características) y produce una salida binaria (0/1, sí/no).</li>
        <li>Se ajusta a partir de ejemplos: si se equivoca, corrige ligeramente sus pesos.</li>
        <li>Funciona bien cuando las clases son separables “con una línea recta” (o un plano en más dimensiones).</li>
      </ul>

      <figure class="media">
        <img src="../img/tema3-perceptron-linea-separadora.JPG" alt="Perceptrón separando dos grupos de puntos mediante una línea" loading="lazy" decoding="async">
        <figcaption>Espacio para una figura con puntos de dos colores y la línea de decisión aprendida por un perceptrón.</figcaption>
      </figure>

      <h3>6.2. Redes MLP (Multi-Layer Perceptron)</h3>
      <p>
        Un <strong>MLP</strong> es una red neuronal formada por varias capas de perceptrones. Suele tener:
      </p>
      <ul>
        <li>Una capa de entrada.</li>
        <li>Una o varias <strong>capas ocultas</strong> con neuronas (perceptrones) totalmente conectadas.</li>
        <li>Una capa de salida que da lugar a la predicción final.</li>
      </ul>
      <p>
        Al añadir capas ocultas, el MLP es capaz de aprender <strong>fronteras de decisión no lineales</strong>, mucho más
        complejas que una simple línea recta.
      </p>

      <figure class="media">
        <img src="../img/tema3-mlp-frontera-curva.JPG" alt="MLP generando una frontera de decisión no lineal" loading="lazy" decoding="async">
        <figcaption>Espacio para una figura con datos en 2D y una frontera de decisión curva aprendida por un MLP.</figcaption>
      </figure>

      <div class="note">
        En la mini-práctica 1 utilizaremos un MLP sencillo para clasificar puntos en 2D
        y ver cómo cambia el rendimiento al modificar el número de neuronas.
      </div>
    </section>

    <footer>
      <p>© 2025 — Material docente DigiTech. Jose Miguel Martínez.</p>
    </footer>
  </div>
  <a class="top" href="#top" aria-label="Volver arriba">↑</a>
</body>
</html>

