<!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Árboles de decisión — Parte 2</title>
  <!-- MISMO CSS QUE LA PARTE 1 -->
  <style>
    :root {
      --fg: #111827; /* slate-900 */
      --muted: #4b5563; /* gray-600 */
      --bg: #ffffff;
      --brand: #0ea5e9; /* sky-500 */
      --brand-ink: #075985; /* sky-800 */
      --card: #f8fafc; /* slate-50 */
      --code: #111827; /* slate-900 */
      --border: #e5e7eb; /* gray-200 */
      /* accent */
      --h1: #1d4ed8; /* blue-700 */
      --h2: #6d28d9; /* violet-700 */
      --h3: #0284c7; /* sky-600 */
      --section-underline: #dbeafe; /* blue-100 */
      --divider: #e5e7eb; /* gray-200 */
    }
    html, body { background: var(--bg); color: var(--fg); font-family: system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji"; }
    .page { max-width: 920px; margin: auto; padding: 2rem 1.25rem 6rem; line-height: 1.65; }
    header h1 { font-size: clamp(1.6rem, 2.5vw + 1rem, 2.4rem); margin: 0 0 .25rem; color: var(--h1); letter-spacing: -0.01em; }
    header p.lead { color: var(--muted); margin: 0 0 1.25rem; }
    .breadcrumbs { font-size: .9rem; color: var(--muted); margin-bottom: .5rem; }
    .breadcrumbs a { color: var(--muted); text-decoration: none; }
    nav.toc { background: var(--card); border: 1px solid var(--border); border-radius: .75rem; padding: 1rem; margin: 1.25rem 0 2rem; }
    nav.toc strong { display: block; margin-bottom: .5rem; }
    nav.toc a { display: block; color: var(--brand-ink); text-decoration: none; padding: .25rem 0; }
    h2 { margin-top: 2.25rem; font-size: 1.6rem; color: var(--h2); padding-bottom: .35rem; border-bottom: 1px solid var(--section-underline); }
    h3 { margin-top: 1.5rem; font-size: 1.25rem; color: var(--h3); }
    .page section + section { border-top: 1px solid var(--divider); margin-top: 2.25rem; padding-top: 2rem; }
    .note { background: #ecfeff; border-left: 4px solid var(--brand); padding: .75rem 1rem; border-radius: .5rem; }
    .card { background: var(--card); border: 1px solid var(--border); border-radius: .75rem; padding: 1rem; }
    code, pre { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; font-size: .95em; }
    pre { background: #0b1020; color: #e2e8f0; border-radius: .5rem; padding: 1rem; overflow: auto; }
    .pill { display: inline-block; border: 1px solid var(--border); border-radius: 999px; padding: .1rem .6rem; font-size: .875rem; color: var(--muted); background: #fff; }
    details { border: 1px solid var(--border); border-radius: .5rem; padding: .75rem 1rem; background: #fff; }
    details + details { margin-top: .75rem; }
    footer { margin-top: 3rem; font-size: .9rem; color: var(--muted); }
    .top { position: fixed; right: 1rem; bottom: 1rem; background: var(--brand); color: #fff; border-radius: 999px; padding: .6rem .8rem; text-decoration: none; box-shadow: 0 6px 18px rgba(2,132,199,.3); }
    figure.media { margin: 1rem 0 1.25rem; }
    figure.media img { max-width: 100%; height: auto; display: block; border-radius: .5rem; }
    figure.media figcaption { margin-top: .5rem; text-align: center; color: var(--muted); font-size: .95rem; }
  </style>
</head>
<body>
  <div class="page">
    <header id="top">
      <div class="breadcrumbs">Sistemas de Aprendizaje Automático → Tema 2 → <span class="pill">Parte 2</span></div>
      <h1>Árboles de decisión</h1>
      <p class="lead">Guía docente con definiciones, ejemplos y espacios para tus propias figuras.</p>
    </header>

    <nav class="toc" aria-label="Tabla de contenidos">
      <strong>Índice de la página</strong>
      <a href="#definicion">1. ¿Qué son y para qué se utilizan?</a>
      <a href="#representacion">2. Representación (nodos, hojas, ramas)</a>
      <a href="#ventajas">3. Ventajas y desventajas</a>
      <a href="#tipos">4. Tipos de árboles: ID3, C4.5 y CART</a>
      <a href="#matriz-confusion">5. Matriz de confusión</a>
      <a href="#svm">6. Máquinas de Vectores de Soporte (SVM)</a>
    </nav>

    <!-- 1 -->
    <section id="definicion">
      <h2>1. ¿Qué son y para qué se utilizan?</h2>
      <p>Un <strong>árbol de decisión</strong> es un modelo supervisado que aprende <em>reglas en forma de preguntas</em> sobre las características para clasificar ejemplos o predecir valores.</p>
      <ul>
        <li><strong>Clasificación:</strong> aprobar/denegar un crédito, detectar spam, diagnosticar una enfermedad.</li>
        <li><strong>Regresión:</strong> estimar precio de vivienda, consumo energético o demanda diaria.</li>
      </ul>
      <figure class="media">
        <img src="../img/parte2-arbol-decision.JPG" alt="Tabla árbol de decisión" width="1200" height="600" loading="lazy" decoding="async">
        <figcaption>Tabla del modelo.</figcaption>
      </figure>
    </section>

    <!-- 2 -->
    <section id="representacion">
      <h2>2. Representación (nodos, hojas, ramas)</h2>
      <ul>
        <li><strong>Nodo interno:</strong> pregunta/regla (p. ej., <code>edad &lt; 30</code>).</li>
        <li><strong>Ramas:</strong> resultados de la regla (sí/no o múltiples valores).</li>
        <li><strong>Hoja:</strong> decisión final (clase) o valor numérico (regresión).</li>
      </ul>
      <div class="note">Consejo: limita <em>profundidad</em>, tamaño mínimo de hojas o usa <em>poda</em> para controlar complejidad.</div>
      <figure class="media">
        <img src="../img/parte2-arbol-decision - Ejemplo .JPG" alt="Ejemplo con nodos, ramas y hojas" width="1100" height="600" loading="lazy" decoding="async">
        <figcaption>Figura — Árbol Generado.</figcaption>
      </figure>
    </section>

    <!-- 3 -->
    <section id="ventajas" class="card">
      <h2>3. Ventajas y desventajas</h2>
      <ul>
        <li><strong>Ventajas:</strong> interpretables, manejan variables numéricas y categóricas, capturan interacciones y no linealidades.</li>
        <li><strong>Desventajas:</strong> sensibles a variaciones en datos, riesgo de <em>sobreajuste</em> si son muy profundos, cortes ortogonales que a veces no capturan fronteras complejas.</li>
      </ul>
      <figure class="media">
        <img src="../img/underfit-goodfit-overfit-1.png" alt="Bajo ajuste, buen ajuste y sobreajuste" width="1200" height="600" loading="lazy" decoding="async">
        <figcaption>Figura — Bajo ajuste vs buen ajuste vs sobreajuste.</figcaption>
      </figure>
      <p class="note">
        Recurso interactivo:
        <a href="https://mlu-explain.github.io/decision-tree/" target="_blank" rel="noopener noreferrer">
          Decision Trees — MLU-Explain
        </a>
        (explicación visual y paso a paso).
      </p>
    </section>

    <!-- 4 -->
    <section id="tipos">
      <h2>4. Tipos de árboles: ID3, C4.5 y CART</h2>

      <h3>4.1. Definiciones simples con ejemplos cotidianos</h3>
      <dl>
        <dt><strong>ID3</strong></dt>
        <dd>
          Elige en cada paso la <em>pregunta</em> que mejor separa las clases según la información que aporta. 
          <ul>
            <li><em>Ejemplo 1:</em> clasificar <strong>emails</strong> (¿contiene “oferta”, ¿tiene adjunto?, ¿dominio conocido?) para decir <strong>spam/no spam</strong>.</li>
            <li><em>Ejemplo 2:</em> decidir si <strong>recomendar una película</strong> según edad, género y si le gustaron títulos similares.</li>
          <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://openaccess.uoc.edu/server/api/core/bitstreams/7339df02-71ce-4f90-8b5b-1caa757d066e/content" target="_blank" rel="noopener noreferrer">
          ID3
        </a>
          </ul>
        </dd>

        <dt><strong>C4.5</strong></dt>
        <dd>
          Evolución de ID3 que maneja <em>números</em> con umbrales, <em>valores perdidos</em> y evita sesgos hacia atributos con muchos valores; además <em>poda</em> ramas poco útiles.
          <ul>
            <li><em>Ejemplo 1:</em> predecir <strong>baja de clientes</strong> usando variables mezcladas (edad, minutos de uso, provincia).</li>
            <li><em>Ejemplo 2:</em> <strong>clasificar frutas</strong> por peso y color incluso si faltan algunos datos.</li>
          </ul>
        </dd>

        <dt><strong>CART</strong></dt>
        <dd>
          Árbol <em>binario</em> muy usado en librerías modernas. Sirve para <strong>clasificación y regresión</strong> (divide con reglas sí/no y luego poda).
          <ul>
            <li><em>Ejemplo 1:</em> aprobar/denegar un <strong>crédito</strong> con reglas sencillas (ingresos, antigüedad, deudas).</li>
            <li><em>Ejemplo 2:</em> estimar el <strong>precio</strong> de una vivienda con divisiones por superficie y zona.</li>
          </ul>
        </dd>
      </dl>

      <figure class="media">
        <img src="../img/tipos-arboles-ejemplos.png" alt="Lámina de ID3, C4.5 y CART con ejemplos" width="1200" height="640" loading="lazy" decoding="async">
        <figcaption>Figura — Comparativa visual rápida.</figcaption>
      </figure>
    </section>

    <!-- 5 -->
    <section id="matriz-confusion">
      <h2>5. Matriz de confusión (definición)</h2>
      <p>Tabla 2×2 para problemas binarios (extensible a multicategoría) que relaciona las <em>predicciones</em> del modelo con las <em>etiquetas reales</em>:</p>
      <pre><code>                 Clase real
               Pos      Neg
Pred Pos   →   TP       FP
Pred Neg   →   FN       TN
      </code></pre>
      <ul>
        <li><strong>TP</strong> verdadero positivo; <strong>FP</strong> falso positivo; <strong>FN</strong> falso negativo; <strong>TN</strong> verdadero negativo.</li>
      </ul>
      <div class="note">De aquí salen métricas como <em>exactitud</em>, <em>precisión</em>, <em>recobrado</em> y <em>F1</em>.</div>

      <figure class="media">
        <img src="../img/matriz-confusion.png" alt="Matriz de confusión con TP, FP, FN, TN" width="1100" height="560" loading="lazy" decoding="async">
        <figcaption>Figura — Deja aquí tu imagen de matriz de confusión.</figcaption>
      </figure>
    </section>

    <!-- 6 -->
    <section id="svm" class="card">
      <h2>6. Máquinas de Vectores de Soporte (SVM)</h2>
      <h3>6.1. ¿Qué es y por qué es supervisado?</h3>
      <p>
        Un <strong>SVM</strong> busca el <em>hiperplano</em> que separa clases con el <strong>máximo margen</strong>. Es un algoritmo <em>supervisado</em> porque 
        aprende a partir de ejemplos etiquetados <code>(X, y)</code> para que la frontera resultante clasifique correctamente esos ejemplos y generalice a nuevos casos.
      </p>

      <h3>6.2. Partes clave</h3>
      <ul>
        <li><strong>Hiperplano</strong>: la frontera de decisión.</li>
        <li><strong>Margen</strong>: distancia entre el hiperplano y los puntos más cercanos.</li>
        <li><strong>Vectores soporte</strong>: puntos que “tocan” el margen y definen la frontera.</li>
        <li><strong>Parámetro C</strong>: controla errores vs. margen (regularización).</li>
        <li><strong>Kernels</strong> (opcional): permiten separar datos no lineales (lineal, polinómico, RBF, etc.).</li>
      </ul>

      <h3>6.3. Ejemplos prácticos</h3>
      <ul>
        <li><strong>Reconocimiento de dígitos</strong> (0–9) con SVM en <em>scikit-learn</em> (dataset de dígitos). 
          <a href="https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html" target="_blank" rel="noopener noreferrer">Ejemplo oficial</a>.
        </li>
        <li><strong>Detección de peatones</strong> con <em>HOG + SVM</em> en OpenCV (clasificador lineal pre-entrenado).
          <a href="https://docs.opencv.org/3.4/d5/d33/structcv_1_1HOGDescriptor.html" target="_blank" rel="noopener noreferrer">Documentación HOGDescriptor</a>.
        </li>
      </ul>

      <h3>6.4. ¿Cuándo preferir SVM vs. Regresión logística o Árboles?</h3>
      <ul>
        <li><strong>SVM vs. Regresión logística</strong>: ambos son lineales en el espacio de entrada, pero la RL optimiza <em>probabilidades</em> (salida calibrada) mientras que SVM optimiza el <em>margen</em> (buena separación incluso en alta dimensión). Si necesitas probabilidades bien calibradas, la RL suele ser preferible; si buscas una frontera robusta y tienes datos de alta dimensión, SVM brilla.</li>
        <li><strong>SVM vs. Árboles</strong>: los árboles son más <em>interpretables</em> y manejan variables mixtas sin estandarizar; SVM (con kernels) puede capturar fronteras complejas con buen rendimiento, pero es menos interpretable y sensible a la escala de las variables.</li>
      </ul>

      <figure class="media">
        <img src="../img/svm-esquema.png" alt="Hiperplano, margen y vectores soporte" width="1200" height="640" loading="lazy" decoding="async">
        <figcaption>Figura — Espacio para una lámina de SVM (hiperplano, margen y vectores soporte).</figcaption>
      </figure>

      <p class="note">
        Más recursos: 
        <a href="https://scikit-learn.org/stable/modules/svm.html" target="_blank" rel="noopener noreferrer">Guía SVM en scikit-learn</a>.
      </p>
    </section>

    <footer>
      <p>© 2025 — Material docente DigiTech. Jose Miguel Martínez.</p>
    </footer>
  </div>
  <a class="top" href="#top" aria-label="Volver arriba">↑</a>
</body>
</html>
