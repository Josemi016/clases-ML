{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c205df",
   "metadata": {},
   "source": [
    "# Árbol de Decisión — Overfitting vs Good Fit vs Underfitting\n",
    "\n",
    "**Dataset**: *Banknote Authentication* (binario: 0=auténtico, 1=falso).\n",
    "\n",
    "- Kaggle (referencia): Bank Note Authentication UCI data. \"https://www.kaggle.com/datasets/ritesaluja/bank-note-authentication-uci-data\"\n",
    "- UCI (descarga directa en el código): *Banknote Authentication Data Set*.\n",
    "\n",
    "**Objetivo didáctico**: entrenar **tres árboles** con distinta complejidad y comparar:\n",
    "1) **Overfitting** (árbol muy profundo)\n",
    "2) **Good fit** (profundidad moderada)\n",
    "3) **Underfitting** (árbol muy poco profundo)\n",
    "\n",
    "Al final, **escribe tus observaciones** y di **cuál elegirías y por qué**.\n",
    "\n",
    "Recursos de apoyo: https://4geeks.com/es/lesson/arboles-de-decision\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d114c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d19cd",
   "metadata": {},
   "source": [
    "## 1) Cargar dataset y quedarnos con 2 features\n",
    "Usaremos **variance** y **skewness** para poder dibujar regiones de decisión fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfd3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n",
    "cols = ['variance','skewness','curtosis','entropy','target']\n",
    "df = pd.read_csv(url, header=None, names=cols)\n",
    "\n",
    "X = df[['variance','skewness']].values\n",
    "y = df['target'].values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffecd12",
   "metadata": {},
   "source": [
    "## 2) División train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a0975",
   "metadata": {},
   "source": [
    "## 3) Función para dibujar regiones de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f825d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, clf, title='Decision regions'):\n",
    "    x_min, x_max = X[:,0].min() - 1.0, X[:,0].max() + 1.0\n",
    "    y_min, y_max = X[:,1].min() - 1.0, X[:,1].max() + 1.0\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 400),\n",
    "        np.linspace(y_min, y_max, 400)\n",
    "    )\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, s=25)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('variance')\n",
    "    plt.ylabel('skewness')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d8e65",
   "metadata": {},
   "source": [
    "## 4) Entrenar tres árboles con distinta complejidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aeaae4",
   "metadata": {},
   "source": [
    "### 4.1 Overfitting (árbol aobreajustado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1058491",
   "metadata": {},
   "outputs": [],
   "source": [
    "over = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=None, min_samples_leaf=1)\n",
    "over.fit(X_train, y_train)\n",
    "y_pred_tr = over.predict(X_train)\n",
    "y_pred_te = over.predict(X_test)\n",
    "acc_tr = accuracy_score(y_train, y_pred_tr)\n",
    "acc_te = accuracy_score(y_test, y_pred_te)\n",
    "cm_over = confusion_matrix(y_test, y_pred_te)\n",
    "print('Accuracy train:', round(acc_tr,3))\n",
    "print('Accuracy test :', round(acc_te,3))\n",
    "print('Confusion matrix (test):\\n', cm_over)\n",
    "plot_decision_regions(X_train, y_train, over, title='Overfitting — decision regions (train)')\n",
    "plot_decision_regions(X_test, y_test, over, title='Overfitting — decision regions (test)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41145647",
   "metadata": {},
   "source": [
    "### 4.2 Good fit (profundidad moderada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c50ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=3, min_samples_leaf=5)\n",
    "good.fit(X_train, y_train)\n",
    "y_pred_tr = good.predict(X_train)\n",
    "y_pred_te = good.predict(X_test)\n",
    "acc_tr = accuracy_score(y_train, y_pred_tr)\n",
    "acc_te = accuracy_score(y_test, y_pred_te)\n",
    "cm_good = confusion_matrix(y_test, y_pred_te)\n",
    "print('Accuracy train:', round(acc_tr,3))\n",
    "print('Accuracy test :', round(acc_te,3))\n",
    "print('Confusion matrix (test):\\n', cm_good)\n",
    "plot_decision_regions(X_train, y_train, good, title='Good fit — decision regions (train)')\n",
    "plot_decision_regions(X_test, y_test, good, title='Good fit — decision regions (test)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299f517",
   "metadata": {},
   "source": [
    "### 4.3 Underfitting (árbol demasiado simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "under = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=1)\n",
    "under.fit(X_train, y_train)\n",
    "y_pred_tr = under.predict(X_train)\n",
    "y_pred_te = under.predict(X_test)\n",
    "acc_tr = accuracy_score(y_train, y_pred_tr)\n",
    "acc_te = accuracy_score(y_test, y_pred_te)\n",
    "cm_under = confusion_matrix(y_test, y_pred_te)\n",
    "print('Accuracy train:', round(acc_tr,3))\n",
    "print('Accuracy test :', round(acc_te,3))\n",
    "print('Confusion matrix (test):\\n', cm_under)\n",
    "plot_decision_regions(X_train, y_train, under, title='Underfitting — decision regions (train)')\n",
    "plot_decision_regions(X_test, y_test, under, title='Underfitting — decision regions (test)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e110c",
   "metadata": {},
   "source": [
    "## 5) Conclusión — Recuadro para tus observaciones\n",
    "- **Overfitting**: ¿qué observas en entrenamiento vs test?\n",
    "- **Good fit**: ¿cómo se comparan las fronteras de decisión entre clases? ¿y las métricas?\n",
    "- **Underfitting**: ¿qué patrón ves? ¿qué está pasando con la frontera?\n",
    "\n",
    "**¿Cuál es el mejor y por qué?**\n",
    "\n",
    "> Escribe aquí tu conclusión final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
